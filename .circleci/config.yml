version: 2.1

commands:
  install_packages:
    steps:
      - run:
          name: Install packages and dependencies
          command: |
            apt-get -y update
            apt-get -y upgrade
            apt-get install -y ffmpeg
            python -m venv venv
            source venv/bin/activate
            pip install poetry
            poetry export -f requirements.txt --output requirements.txt
            pip install -r requirements.txt

  export_environment_to_file:
    steps:
      - run:
          name: Export environment variables
          command: |
            touch .env
            echo "API_KEY=$API_KEY" >> .env
            echo "AWS_REGION_NAME=$AWS_REGION_NAME" >> .env
            echo "AWS_S3_ARTWORK_BUCKET=$AWS_S3_ARTWORK_BUCKET" >> .env
            echo "AWS_S3_MUSIC_BUCKET=$AWS_S3_MUSIC_BUCKET" >> .env
            echo "GOOGLE_API_KEY=$GOOGLE_API_KEY" >> .env
            echo "GOOGLE_CLIENT_ID=$GOOGLE_CLIENT_ID" >> .env
            echo "GOOGLE_CLIENT_SECRET=$GOOGLE_CLIENT_SECRET" >> .env
            echo "MIGRATION_DATABASE_URL=$MIGRATION_DATABASE_URL" >> .env
            echo "POSTGRES_DB=$POSTGRES_DB" >> .env
            echo "POSTGRES_HOST=$POSTGRES_HOST" >> .env
            echo "POSTGRES_PASSWORD=$POSTGRES_PASSWORD" >> .env
            echo "POSTGRES_USER=$POSTGRES_USER" >> .env
            echo "SECRET_KEY=$SECRET_KEY" >> .env
            echo "SERVER_PORT=$SERVER_PORT" >> .env
            echo "ENV=$ENV" >> .env
            echo "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" >> .env
            echo "AWS_ENDPOINT_URL=$AWS_ENDPOINT_URL" >> .env
            echo "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" >> .env
            echo "REDIS_URL=$REDIS_URL" >> .env

jobs:
  build and test:
    docker:
      - image: python:3.10
      - image: redis
    steps:
      - checkout
      - install_packages
      - run:
          name: Use test environment variables
          command: |
            echo 'AWS_ACCESS_KEY_ID=$TEST_AWS_ACCESS_KEY_ID' >> $BASH_ENV
            echo 'AWS_ENDPOINT_URL=$TEST_AWS_ENDPOINT_URL' >> $BASH_ENV
            echo 'AWS_SECRET_ACCESS_KEY=$TEST_AWS_SECRET_ACCESS_KEY' >> $BASH_ENV
      - run:
          name: Run Tests
          command: |
            source venv/bin/activate
            pytest -s
          environment:
            ENV: testing

  deploy:
    docker:
      - image: python:3.10
    steps:
      - checkout
      - run:
          name: Install rsync
          command: |
            apt-get -y update
            apt-get install -y rsync
      - run:
          name: Backup container data
          command: |
            ssh -o StrictHostKeyChecking=no -v $SSH_USER@$SSH_HOST "
              set -e
              [ -d ~/data ] || mkdir ~/data
              docker cp server:/src/logs ~/data/server
              docker cp worker:/src/logs ~/data/worker
              docker cp redis:/data ~/data/redis
              docker cp postgres:/var/lib/postgresql/data ~/data/postgresql
              [ -d ~/data/backup ] || mkdir ~/data/backup
              docker exec postgres pg_dump -U dripdrop dripdrop > ~/data/backup/db.sql
            "
      - export_environment_to_file
      - run:
          name: Sync repo to VM
          command: rsync -arzP 'ssh -o StrictHostKeyChecking=no' $(pwd)/ $SSH_USER@$SSH_HOST:~/dripdrop --exclude=data
      - run:
          name: Deploy Over SSH
          command: |
            ssh -o StrictHostKeyChecking=no -v $SSH_USER@$SSH_HOST "
              set -e
              cd ~/dripdrop
              docker compose -f docker-compose.yml build --progress plain server
              docker compose -f docker-compose.yml up --no-deps -d server
            "

workflows:
  version: 2
  deploy:
    jobs:
      - build and test
      - deploy:
          requires:
            - build and test
          filters:
            branches:
              only: main # only deploy on the main branch
